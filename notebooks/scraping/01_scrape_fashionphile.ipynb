{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd9b1f-226e-4738-8403-0bcd083a5a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "01_scrape_fashionphile.ipynb\n",
    "\n",
    "Goal:\n",
    "- Collect product URLs from a Fashionphile listing page (Selenium, because listing is JS-heavy)\n",
    "- For each product URL, scrape structured fields (Requests + BeautifulSoup)\n",
    "- Save results to CSV\n",
    "\n",
    "Notes:\n",
    "- Be polite: add delays and timeouts\n",
    "- Keep functions small + testable\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ff006-7ed0-4059-a08a-e2be5ae69641",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/123.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "REQUEST_TIMEOUT = 20\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "DEFAULT_DELAY_SECONDS = 1.0   # be polite\n",
    "JITTER_SECONDS = 0.25         # add small randomness to delays\n",
    "\n",
    "# Listing page CSS selector for product cards (verify if it changes)\n",
    "LISTING_CARD_SELECTOR = \"a.full-unstyled-link.fp-card__link\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e4ddc-52c0-463b-a4cd-34ffb9df3c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url: str, headers: dict = HEADERS, timeout: int = REQUEST_TIMEOUT) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Fetch a URL with Requests and return BeautifulSoup HTML parser.\n",
    "    Includes retries for transient errors.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            return BeautifulSoup(resp.text, \"html.parser\")\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(0.5 * attempt)  # backoff\n",
    "    raise last_err\n",
    "\n",
    "\n",
    "def get_text(soup: BeautifulSoup, selector: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Return stripped text for the first matching CSS selector, or None.\"\"\"\n",
    "    if not selector:\n",
    "        return None\n",
    "    el = soup.select_one(selector)\n",
    "    return el.get_text(strip=True) if el else None\n",
    "\n",
    "\n",
    "def clean_price(text: Optional[str]) -> Optional[float]:\n",
    "    \"\"\"Convert '$1,234.00' -> 1234.0 safely.\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    digits = \"\".join(ch for ch in text if ch.isdigit() or ch == \".\")\n",
    "    try:\n",
    "        return float(digits)\n",
    "    except ValueError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6e352-d78a-459f-915c-5d237c2543a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTORS = {\n",
    "    \"title\": \"h1\",\n",
    "    \"model\": \"p.fp-product-title__details\",\n",
    "    \"sell_price\": \"span.price-item--regular\",\n",
    "    \"condition\": \"span.h6.fp-font-weight--regular\",\n",
    "    # sku/status are extracted from accordion text blocks\n",
    "    \"sku\": None,\n",
    "    \"status\": None,\n",
    "    \"brand\": \"a.fp-product-vendor__link\",\n",
    "}\n",
    "\n",
    "\n",
    "def infer_leather(model: Optional[str], desc: Optional[str]) -> Optional[str]:\n",
    "    text = \" \".join([model or \"\", desc or \"\"]).lower()\n",
    "    for name in [\"togo\", \"epsom\", \"clemence\", \"swift\", \"box\", \"chevre\"]:\n",
    "        if name in text:\n",
    "            return name.capitalize()\n",
    "    return None\n",
    "\n",
    "\n",
    "def infer_hardware(desc: Optional[str]) -> Optional[str]:\n",
    "    if not desc:\n",
    "        return None\n",
    "    t = desc.lower()\n",
    "    if \"palladium hardware\" in t:\n",
    "        return \"Palladium\"\n",
    "    if \"gold hardware\" in t:\n",
    "        return \"Gold\"\n",
    "    if \"silver hardware\" in t:\n",
    "        return \"Silver\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def infer_color_from_model(model: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Very naive: uses last token in the model string.\n",
    "    Example: \"HERMES TOGO BIRKIN 30 TRENCH\" -> \"TRENCH\"\n",
    "    \"\"\"\n",
    "    if not model:\n",
    "        return None\n",
    "    parts = model.split()\n",
    "    return parts[-1] if parts else None\n",
    "\n",
    "\n",
    "def infer_size_from_model(model: Optional[str]) -> Optional[int]:\n",
    "    \"\"\"Extract the first integer token from model string (e.g., 'Birkin 30' -> 30).\"\"\"\n",
    "    if not model:\n",
    "        return None\n",
    "    for token in model.split():\n",
    "        if token.isdigit():\n",
    "            return int(token)\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_description_from_soup(soup: BeautifulSoup) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Pull the long product description from accordion content blocks.\n",
    "    This is heuristic and depends on Fashionphile HTML structure.\n",
    "    \"\"\"\n",
    "    blocks = soup.select(\"div.accordion__content.rte.body-md\")\n",
    "    for div in blocks:\n",
    "        text = div.get_text(\" \", strip=True)\n",
    "        lower = text.lower()\n",
    "        if \"this is an authentic\" in lower or lower.startswith(\"this is\"):\n",
    "            idx = lower.find(\"this is\")\n",
    "            return text[idx:] if idx != -1 else text\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_sku_from_soup(soup: BeautifulSoup) -> Optional[str]:\n",
    "    \"\"\"Extract SKU from accordion blocks via regex like 'Item #: 1747041'.\"\"\"\n",
    "    blocks = soup.select(\"div.accordion__content.rte.body-md\")\n",
    "    for div in blocks:\n",
    "        text = div.get_text(\" \", strip=True)\n",
    "        cleaned = \" \".join(text.split())\n",
    "        match = re.search(r\"Item\\s*#:\\s*(\\d+)\", cleaned, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_brand(soup: BeautifulSoup) -> Optional[str]:\n",
    "    el = soup.select_one(\"a.fp-product-vendor__link\")\n",
    "    return el.get_text(strip=True) if el else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a515c5d2-e48a-4c43-b43a-a66181f0f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_product_page(url: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Scrape one product page and return a normalized dictionary row.\n",
    "    \"\"\"\n",
    "    soup = get_soup(url)\n",
    "\n",
    "    # Basic fields via CSS selectors\n",
    "    data = {field: get_text(soup, sel) for field, sel in SELECTORS.items()}\n",
    "\n",
    "    # Normalize / enrich\n",
    "    data[\"sell_price\"] = clean_price(data.get(\"sell_price\"))\n",
    "\n",
    "    desc = extract_description_from_soup(soup)\n",
    "    data[\"description\"] = desc\n",
    "\n",
    "    data[\"sku\"] = extract_sku_from_soup(soup)\n",
    "\n",
    "    model = data.get(\"model\")\n",
    "    data[\"leather\"] = infer_leather(model, desc)\n",
    "    data[\"hardware\"] = infer_hardware(desc)\n",
    "    data[\"color\"] = infer_color_from_model(model)\n",
    "    data[\"size_cm\"] = infer_size_from_model(model)\n",
    "\n",
    "    # Brand extraction (override selector field to be safe)\n",
    "    data[\"brand\"] = extract_brand(soup)\n",
    "\n",
    "    data[\"url\"] = url\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e71373-3137-4767-9449-2a092535b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_driver(headless: bool = True) -> webdriver.Chrome:\n",
    "    \"\"\"Create a Chrome webdriver (headless by default).\"\"\"\n",
    "    options = Options()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    driver = webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=options\n",
    "    )\n",
    "    return driver\n",
    "\n",
    "\n",
    "def make_page_url(base_url: str, page: int) -> str:\n",
    "    \"\"\"\n",
    "    Return URL for a given page number.\n",
    "    Assumes the listing uses '&page=N'. If the base URL changes, update here.\n",
    "    \"\"\"\n",
    "    if page == 1:\n",
    "        return base_url\n",
    "    return f\"{base_url}&page={page}\"\n",
    "\n",
    "\n",
    "def get_product_links_from_listing_selenium(\n",
    "    listing_url: str,\n",
    "    driver: webdriver.Chrome,\n",
    "    max_products: Optional[int] = None,\n",
    "    wait_seconds: int = 30\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Load a listing page and return product links found.\n",
    "    \"\"\"\n",
    "    driver.get(listing_url)\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, wait_seconds).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, LISTING_CARD_SELECTOR))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Timed out waiting for product cards:\", e)\n",
    "\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, LISTING_CARD_SELECTOR)\n",
    "    links = []\n",
    "\n",
    "    for el in elements:\n",
    "        href = el.get_attribute(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        if \"/products/\" not in href:\n",
    "            continue\n",
    "        links.append(href)\n",
    "\n",
    "    # de-dupe while preserving order\n",
    "    links = list(dict.fromkeys(links))\n",
    "\n",
    "    if max_products is not None:\n",
    "        links = links[:max_products]\n",
    "\n",
    "    print(f\"Found {len(links)} product links on this page.\")\n",
    "    return links\n",
    "\n",
    "\n",
    "def get_all_product_links_across_pages(\n",
    "    base_url: str,\n",
    "    driver: webdriver.Chrome,\n",
    "    max_pages: int = 10,\n",
    "    max_total: Optional[int] = None\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Paginate the listing and collect product links.\n",
    "    Stops if a page returns zero links.\n",
    "    \"\"\"\n",
    "    all_links = []\n",
    "    seen = set()\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        page_url = make_page_url(base_url, page)\n",
    "        print(f\"\\n=== Page {page} â†’ {page_url} ===\")\n",
    "\n",
    "        links = get_product_links_from_listing_selenium(page_url, driver)\n",
    "\n",
    "        if not links:\n",
    "            print(\"No products found on this page. Stopping pagination.\")\n",
    "            break\n",
    "\n",
    "        for u in links:\n",
    "            if u not in seen:\n",
    "                seen.add(u)\n",
    "                all_links.append(u)\n",
    "\n",
    "        print(f\"Total collected so far: {len(all_links)}\")\n",
    "\n",
    "        if max_total is not None and len(all_links) >= max_total:\n",
    "            all_links = all_links[:max_total]\n",
    "            print(f\"Reached max_total={max_total}. Stopping.\")\n",
    "            break\n",
    "\n",
    "    return all_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfd74d-29d2-44fc-a347-c95131db9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_products_to_df(\n",
    "    product_links: List[str],\n",
    "    delay: float = DEFAULT_DELAY_SECONDS\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape product pages into a dataframe.\n",
    "    Includes delay + jitter to reduce risk of rate-limits.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    total = len(product_links)\n",
    "\n",
    "    for i, url in enumerate(product_links, start=1):\n",
    "        print(f\"{i}/{total} {url}\")\n",
    "        try:\n",
    "            row = parse_product_page(url)\n",
    "            rows.append(row)\n",
    "        except Exception as e:\n",
    "            print(\"   Error parsing:\", repr(e))\n",
    "\n",
    "        # polite delay\n",
    "        time.sleep(delay + random.random() * JITTER_SECONDS)\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a43b0-3bf3-422e-9eb1-803207960110",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LISTING_URL = (\n",
    "    \"https://www.fashionphile.com/collections/all-bags?\"\n",
    "    \"refinementList%5Bvendor%5D%5B0%5D=Hermes&sortBy=shopify_products_published_at_desc\"\n",
    ")\n",
    "\n",
    "driver = make_driver(headless=True)\n",
    "\n",
    "try:\n",
    "    all_links = get_all_product_links_across_pages(\n",
    "        BASE_LISTING_URL,\n",
    "        driver,\n",
    "        max_pages=10,\n",
    "        max_total=1000\n",
    "    )\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "print(\"Total links:\", len(all_links))\n",
    "all_links[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3edca6-69a2-45b5-a71f-4473cad1c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_products_to_df(all_links, delay=1.0)\n",
    "df.shape, df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e165484-52b8-4e7a-9c2d-0eeba7c535a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outputs\n",
    "df.to_csv(\"fashionphile_hermes_bags.csv\", index=False)\n",
    "pd.Series(all_links).to_csv(\"fashionphile_hermes_links.csv\", index=False)\n",
    "\n",
    "df.isna().mean().sort_values(ascending=False).head(15)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
